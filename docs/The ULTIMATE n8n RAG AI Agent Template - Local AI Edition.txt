(Transcrito por TurboScribe.ai. Atualize para Ilimitado para remover esta mensagem.)

Retribuição de geração aumentada é o método mais amplamente adotado para dar acesso aos agentes à sua base de conhecimento, essencialmente fazendo-os expertos de domínio em seus documentos para Q&A. O grande problema, porém, é que o RAG, por si mesmo, é muitas vezes não poderoso o suficiente, e é por isso que no meu canal, no último mês, eu estou ensinando bastante sobre o RAG agêntico, que é uma das formas mais poderosas de aumentar o RAG, fazendo-o mais poderoso e mais flexível para trabalhar com os diferentes tipos de documentos que você tem. Recentemente, eu mostrei o meu template do RAG agêntico N8n, que você pode acessar gratuitamente agora mesmo, e é um ótimo ponto de partida para você construir seu próprio sistema RAG agêntico sem código.

No entanto, neste vídeo, um número de vocês perguntaram-me para fazer uma versão local do RAG agêntico. Quer dizer, literalmente senti-me como se mais de metade dos comentários neste vídeo fossem vocês pedindo isso. Tipo, hey Cole, isso é legal e tudo, mas você pode pular para a parte em que você faz tudo local? E eu estava secretamente esperando por isso, porque eu realmente acredito que o AI local, onde você controla todos os seus dados e seus LLMs, é a direção que precisamos ir para o AI generativo como um todo.

Então, agora mesmo, eu estou te apresentando a versão local deste RAG agêntico N8n. Eu vou passar pelo workflow, os casos de uso que ele cobre, e mostrar-lhe como setar tudo isso, para que você tenha seu próprio RAG agêntico 100% offline, seguro e poderoso. E há alguns quirks com trabalhar com LLMs locais e N8n que eu vou cobrir mais tarde neste vídeo, para te salvar de uma grande dor de cabeça.

Então, definitivamente, fique atento a isso. Então, no último vídeo do meu canal, quando eu cobri a versão de cloud deste template, eu obviamente falei muito sobre o que é um RAG agêntico e por que é tão poderoso. Mas desde então, eu venho com um jeito ainda melhor de explicá-lo, para fazê-lo mais claro e mais conciso para você.

E é usando este artigo do Weave8. Então, ótimo recurso que eu vou ter o link na descrição deste vídeo, pesquisando o que é um RAG agêntico naivo ou tradicional, e depois olhando como ele evoluiu para um RAG agêntico. Então, eles têm um par de diagramas que eu quero passar por aqui, que nos armará com a informação para então ir para o workflow de N8n e entender tudo lá.

O primeiro diagrama aqui é um exemplo de RAG naivo ou tradicional. Vamos ver como isso se esvazia mais tarde para um RAG agêntico. Então, como isso funciona é que você pega todos os seus documentos, você os divide em pedaços, então você tem a informação de tamanho de bota para alimentar um LLM, e depois você usa um modelo de embedição para transformar toda essa informação em vectores para armazenar em um database de vectores, como Superbase, Quadrant ou Pinecone.

E depois você pega a pergunta do usuário, quando essas análises chegam, e você usa o mesmo modelo de embedição para transformar a pergunta em um vector também. Então você pode usar algumas matemáticas de vectores atrás das cenas para encaixar a pergunta com os pedaços do documento que têm a informação mais relevante. E então tudo isso se combina em um único pronto para o LLM.

Então você está dando algo como um pronto de sistema com instruções, a pergunta do usuário, e depois os pedaços relevantes que foram recebidos, e então, esperamos, a resposta que você recebe do LLM tem a resposta porque os pedaços tiveram a informação certa para responder a pergunta. Agora, o problema com tudo isso é que é considerado um pronto. Você olha para a informação no database de vectores e você faz isso apenas uma vez, e há apenas a oportunidade de fazer isso uma vez.

O modelo de linguagem grande no fundo aqui nunca tem a oportunidade de pensar em si mesmo. Oh, eu realmente quero reescrever a pergunta do usuário, ou talvez haja um database de vectores diferente que eu queira procurar, ou talvez eu queira olhar para o dados de uma forma diferente. Não tem jeito de reasonar sobre como ele explora a base de conhecimento, e então é quando queremos graduar-nos a um REG GENTIC.

Agora vamos para este diagrama, onde temos a implementação de um REG GENTIC, e as ferramentas que vemos aqui não correspondem exatamente ao que eu tenho no meu workflow N8n, mas é só porque há diferentes maneiras de implementar um REG GENTIC. E então, tanto isso quanto o meu workflow N8n são bons exemplos, e isso ainda ilustra o que eu estou tentando mostrar muito claramente. Em vez de haver uma recuperação antes de alcançarmos o LLM, com um REG GENTIC, o agente vem primeiro, e então ele tem ferramentas disponíveis para ele performar o REG, e então também, isso é uma coisa muito importante, explorar a base de conhecimento de outras maneiras.

E então, neste exemplo, ele pode procurar através de múltiplos databases de vectores, e ele pode decidir quando ele quer olhar em cada um. Então nós temos muito mais razão cozida neste processo de recuperação, e ele tem outras ferramentas, como a websearch, para procurar informação de maneiras diferentes. E como nós veremos no meu workflow N8n, nós também podemos olhar a base de conhecimento de maneiras diferentes, então não estamos sempre fazendo uma recuperação baseada em matemáticas de vectores.

Uma quantidade infinita de possibilidades disponíveis para nós, para as diferentes maneiras de olhar a base de conhecimento, usando o agente para razoar sobre isso, em vez de ser apenas um tipo de precursor de trabalho para tomar informação de um database de vectores e derrubá-lo para o agente. E então, isso é o que é o REG GENTIC como um todo. Agora nós podemos descer em nosso workflow, armado com essa informação, e dar uma olhada na nossa implementação específica.

Então aqui está o completo template local do REG GENTIC N8n, e você vai notar que isso é bem parecido com a versão de cloud que eu construí recentemente no meu canal. E eu estou fazendo isso com muita intenção, porque não fixe o que não está quebrado, certo? Tipo, se eu te dei um bom template, como eu prometi, então quando nós vamos para a versão local, nós queremos mantê-lo o mais parecido possível, e simplesmente mudar para os serviços de AI locais, como o Ollama e uma superbase self-gestionada. Então é isso que nós estamos fazendo aqui.

E o que faz isso um REG GENTIC, deixe-me zoomar nas ferramentas de agentes aqui. Nós estamos dando ao agente a capacidade de explorar a base de conhecimento de diferentes maneiras, com todas essas ferramentas que nós temos, para interagir com Postgres, que isso pode ser Postgres de vanila, ou uma superbase self-gestionada também, para uma implementação local. Então vamos passar por alguns exemplos agora, e ver o que isso parece.

Veja como ele lidera essas ferramentas diferentes para responder perguntas que um REG naivo normalmente não conseguiria sozinho. Então para o nosso teste aqui, o LLM local que estou usando é o Quen 2.5-14b. Então é um modelo relativamente pequeno, e eu tenho uma versão com um limite de 8.000 tokens de contexto, que eu vou mostrar como instalar mais tarde, porque o Ollama, por default, limita-se a apenas 2.000 tokens, o que não é suficiente para sistemas como este. Então esse é um dos grandes problemas que eu espero que você resolva mais tarde.

Mas agora, vamos lá e testar isso. Então eu quero perguntar algumas perguntas para forçar-o a usar essas ferramentas diferentes para explorar a base de conhecimento de diferentes maneiras, baseado na pergunta que estamos pedindo. Então eu tenho o Claude gerar todo esse dado falso para uma empresa de tecnologia maritima.

E então, para o primeiro teste, eu só quero fazer uma retratação básica de REG. Então, acessando aqui, eu só quero perguntar quem é o CEO dessa empresa. E então, eu estou esperando para usar esse atalho na direita para um simples lookup de REG, porque não há nada mais complexo, é uma pergunta muito básica, vamos testá-lo simplesmente primeiro.

Eu só vou perguntar quem é o CEO da empresa. E podemos ver que com o workflow do NAN, quando ele decide usar essas ferramentas, nós temos essa caixa vermelha ao redor. Então, neste caso, ele não decidiu usar essas outras ferramentas que nós temos para isso, como listando documentos, recebendo conteúdos de arquivos, querendo nossas linhas, mas ele fez o lookup de REG e nos deu a resposta certa, Dr. Elisa Vanderhoven, essa é a resposta certa.

Agora, para o segundo teste, eu quero perguntar uma pergunta onde REG pode funcionar, porque eu não tenho muitos documentos neste ambiente de teste, mas seria o tipo de pergunta que REG pode ser errada. Então, voltando aqui, eu tenho um chat novo e eu vou perguntá-lo quem estava no encontro do 12 de fevereiro. E a razão pela qual eu estou perguntando é porque quando você tem um monte de notas de encontro para dias diferentes em uma base de conhecimento, muitas vezes REG não faz bem porque ele pula a informação das notas de encontro erradas, especialmente se o pedaço para os atendentes está longe da informação que diz o dia do encontro.

Então, eu vou perguntá-lo isso e nós veremos que, em vez de performar REG, esperamos que ele use um desses outros ferramentas. E sim, com certeza, ele fez. Então, ele usou o ferramenta para listar todos os documentos disponíveis na base de conhecimento, e então, baseado nos títulos, ele escolheu um pouco para ler, para levar esse documento inteiro para usar para responder a nossa pergunta.

E então, esperamos que o que ele escolheu para ler é, sim, ele escolheu ler o documento de minutos de encontro, que é, se eu voltar aqui, este aqui, então nosso encontro do 12 de fevereiro, e esperamos que ele nos dê essa lista de atendentes. Vamos voltar e ver se conseguimos a resposta certa. E sim, com certeza, nós fizemos.

Essa é a resposta certa. Tudo bem, está indo bem. E sim, voltando aqui, essa lista corresponde exatamente.

Agora, para o meu tipo de teste favorito, nós vamos testar com dados tabulares, porque REG geralmente faz absolutamente terrível com tablas como essa, especialmente se nós tivermos mais do que apenas um ambiente de teste aqui, onde nós temos milhares de recordes em nossas tablas, porque, um, REG apenas pula pedaços relevantes, e esses pedaços podem ser apenas uma parte da tabla, então só você tem todos os recordes disponíveis para fazer análise, como computar o rating geral, por exemplo, e também o LLM suja na matemática, então, por si mesmos, apenas olhando para o dado em um prompt, ele não vai poder calcular a média muito bem. E então, o que eu fiz com esse template, esse tool aqui, na verdade, permite ao agente tratar os dados Excel e CSV como tablas SQL, então ele pode escrever queries SQL para fazer coisas como computar o rating geral, vamos ver isso em ação aqui, essa é a espécie de coisa que o REG falaria horrivelmente com, de novo, especialmente se minhas tablas fossem muito maiores, então eu o pergunto, qual é o rating geral geral baseado no feedback do cliente? Não tem uma espalhada perfeita lá, totalmente bom, não importa, vamos ver ele usar esse tool que ele ainda não fez, porque ele vai executar uma query SQL, e uma vez que executa, eu vou clicar nele para que você possa ver, não é a mais trivial query SQL, então é muito impressionante que os LLMs podem fazer isso, especialmente as pequenas, como QEN 2.514B, isso é a melhor parte dessa coisa, não é nem tão grande como um LLM. Tudo bem, e aí vamos nós, ele executou a query, levou um pouco lá, então eu tive que pausar e voltar, mas sim, olha, essa é a query recebendo o rating geral geral, e a resposta é 7.89, e essa é a resposta correta, e ele nos deu de volta, absolutamente perfeito, e uma coisa que eu realmente quero chamar a atenção é que seus resultados serão inconsistentes dependendo do modelo de língua grande local que você usa, porque enquanto você está usando esses realmente, realmente pequenos, é claro que os resultados vão ser muito diferentes do que se você está usando a versão do cloud deste template, nós estamos usando todas essas super poderosas LLMs, como Clod 3.7 Sonnet ou GBT 4.0, mas ainda é tão legal o tipo de poder que podemos ter com a AI local, e com este conjunto, nós estamos 100% seguros e offline, essa é a vantagem da AI local, e então, agora estou zoomado aqui, eu quero passar por este todo o funcionamento para você e mostrar para você como instalar, e eu construí este para ser parte do pacote AI local que eu estou trabalhando no meu canal, e então, eu tenho o workflow de JSON para este template para você acessar disponível aqui neste repositório, mas eu vou mostrar para você enquanto estamos instalando as credências, como você pode conectar coisas fora do pacote AI local, assim como dentro dele.

O fã do vídeo de hoje é Cartesia, e eu estou genuinamente animado para trazê-los para você hoje, porque eles estão fazendo algumas grandes ondas no espaço AI voz, não há negação que a AI está completamente revolucionando tanto o conteúdo de voz quanto os produtos, educação online, botes de serviço de clientes, livros de áudio, e até jogos de vídeo. A necessidade, agora mesmo, para a tecnologia de voz natural e responsiva está crescendo e crescendo, mas o grande problema com a maioria das soluções de AI de voz agora mesmo é que elas são muito lentas, muito imprevisíveis ou muito robóticas, e Cartesia está lidando com todos esses, e eles estão fazendo isso para agentes de voz, clonamento de voz, mudança de voz e texto para palavra. Eles também estão a lançar Sonic 2.0, o seu próximo modelo que é um mudador de jogos completo.

É baseado em uma arquitetura de modelo revolucionário de espaço-estado, e ele traz vozes que são incrivelmente realistas, e até a versão anterior do seu modelo foi em segundo lugar em um monte de avaliações de terceira parte apenas atrasando OpenAI, e então, quem sabe, Sonic 2.0 pode tomar o crono agora. E o que faz Sonic 2.0 tão incrível é tanto a velocidade, tão baixo quanto 40 milissegundos para a latência, e controle imprecedente para mudar coisas como emoções, acentos e velocidade com precisão remarcável. E então, no Cartesia Playground, você pode clonar sua própria voz com apenas 3 segundos do seu áudio.

É simplesmente incrível. E então, eu estou gravando minha voz aqui, eu vou dar cerca de 10 segundos, e então nós vamos ter algo que é exatamente tão bom como outras plataformas, como Eleven Lab. É simplesmente incrível.

Então eu vou parar, dar a minha voz um nome, selecionar a minha língua e então cloná-la, e então em alguns segundos nós teremos uma voz pronta para a gente provar, que você também pode invocar através de um API também. Então eu vou escrever um sample prompt. Apenas vá em frente e escute isso.

... com AI local é um absoluto mudador de jogos. Tipo, isso definitivamente soa como eu. Eu estou muito impressionado.

E com os dois modelos primários em sua família, Cartesia está lidando todos os maiores problemas que nós temos com AI de voz agora, porque eles têm Sonic Turbo, quando você quer latências incrivelmente baixas, e então Sonic 2, quando você quer um modelo super controlável e muito acurado e preciso. Então, se você é um criador, desenvolvedor ou proprietário procurando AI de voz eu recomendo muito ver Cartesia. Então, eu vou ter um link na descrição para cartesia.ai. Você pode se inscrever para um acounto gratuito agora para descobrir quão poderoso é para você.

Então o primeiro passo para realmente estabelecer qualquer agente de AI é ter sua database pronta. E eu faço isso realmente fácil para fazer neste template. E sua database é tão importante, porque guarda sua história de conversa e sua base de conhecimento inteira.

E então, olhando para a caixa vermelha, eu tenho um par de nodos que você pode usar para criar tudo para você. E então, a primeira coisa que você tem que fazer é, obviamente, estabelecer seus credenciais Postgres, que eu mostrei como fazer isso para o pacote local de AI antes. É o que eu estou usando agora.

Então, o meu guia é DB. É assim que eu referencio o serviço de database de Superbase no meu stack composto de Docker. E então, para o resto da informação, você só recebe isso das variáveis de ambiente que você estabeleceu no pacote local de AI.

Então, nós temos nossa database de uso, que é Postgres, você seleciona seu password, e então, o porto para Postgres é 5432. E então, se você não está usando o pacote local de AI, você ainda pode fazer tudo nesse templado. Você só tem que mudar o host do DB para o que for seu host local de Postgres ou Superbase host.

E então, uma vez que você tem seu credencial instalado, você pode testá-lo direto aqui. Parece bom para mim. E então, nós podemos continuar executando nosso primeiro relatório de criação de tabela.

E esse é o que para criar nosso metadata documental. Agora, eu vou entrar em Superbase rapidamente e mostrar isso para você. A tabela de metadata documental mostra a informação de nível alto para nossos documentos.

É assim que o LLM é capaz de usar aquela ferramenta. Eu vou para essa parte aqui. Use a ferramenta para listar os documentos que estão disponíveis na base de conhecimento, para que ele saiba qual deles para retirar o conteúdo inteiro quando ele quiser fazer isso em vez de reg.

E então, nós também temos essa coluna de esquema aqui, que eu vou entrar em mais depois. Para nossos fios de tabela, nós queremos dizer ao LLM as colunas que estão disponíveis, para que ele saiba escrever aquelas ferramentas de SQL para trabalhar com as colunas especificamente para o fio, como o feedback do cliente ou o pipeline de venda. E então, isso é o que a metadata é para.

E então, nós temos a nossa tabela para fios de documentos. E então, em vez de criar uma tabela de SQL separada para cada fio tabular que nós temos, em vez disso, nós o guardamos tudo na mesma tabela. E nós estamos usando esse fio de dados JSONB.

Isso é como, não importa a estrutura da tabela, nós somos capazes de guardar todos os recordes aqui nesta coluna. E então, para um fio, nós temos essas colunas, como as notas, e o status, e o custo. E então, indo para outro fio CSV que eu coloquei aqui, nós temos a data, ID do fio, nome do fio.

Então, esses fios todos têm estruturas diferentes, mas nós podemos guardá-los todos em fios de documentos como este. E então, nós criemos, apenas baseado nessa coluna de dados de fios, quando o agente cria aquelas fios de SQL para analisar certos fios, como os recordes de manutenção, por exemplo. E então, isso é tudo para criar suas tabelas.

A outra tabela que nós temos é a tabela de documentos, e é aqui que nós realmente guardamos nossos embedidos para RAG. E a coisa legal sobre o node PostgresPGVectorStore, este é o nosso node RAG, ele cria esta tabela para nós automaticamente. Então, nós nem precisamos ter um terceiro node aqui para construir isso.

Eu diria que a coluna de texto que Postgres cria nesta tabela é diferente do que se você está usando o node SuperbaseVector, ele usa a coluna de conteúdo. Por isso, eu tenho uma que é especificamente chamada DocumentsPG aqui. Então, se você seguiu meus tutoriais antes, e você está usando o SuperbaseVectorStore node, só saiba que você vai ter que recriá-lo, porque é um pouco diferente com Postgres.

Mas eu queria usar Postgres para este todo o funcionamento, porque dessa forma, você não está limitado para Superbase. Eu sei que Superbase é preferido por maioria das pessoas, mas também Postgres é muito mais leve. Então, eu queria cobrir ambas aqui.

Agora, antes que nós possamos gerar nosso agente, agora que nós temos o nosso set-up de databases, nós precisamos setar nossa base de conhecimento, e nós fazemos isso com este pipeline RAG na caixa azul. Esta é a maior parte do funcionamento, porque o que nós estamos fazendo é que estamos tomando nossos fios locais, portanto, o trigger do fio local, e adicioná-los para nossa base Superbase ou base de conhecimento Postgres. Então, eu já tenho este funcionamento executado aqui, só para que eu possa mostrar os inputs e outputs para todos os nodes enquanto nós passamos por eles bem rapidamente aqui.

Então, nós estamos olhando para fios que foram adicionados ou mudados. Você pode adicionar e deletar se quiser também. Eu só estou mantendo isso simples.

E nós estamos olhando neste folhado na nossa máquina, especificamente. E a razão pela qual eu escolhi este caminho é porque, usando o paquete local AI, eu tenho este folhado compartilhado na minha máquina montado no conteineiro N8n. É o que é, por defaulto.

Então, é provavelmente o caso para você também. Então, nós temos todos os nossos dados de teste, alguns dos quais nós vimos antes. Isso está agora tudo no conteineiro N8n.

E você quer incluir essas opções também para selecionar e linkar fios e folhas. Senão, este trigger não vai funcionar. Ele só vai girar para sempre, esperando para você adicionar ou editar fios.

E eu já vi em um computador Mac, especificamente, às vezes, essa questão permanece, mesmo com essas opções selecionadas. Subir um bug para N8n, se você vê isso, eu sei que é algo que eles estão trabalhando. Mas, de qualquer forma, uma vez que nós temos este trigger, então nós temos um evento e um caminho.

Então, o caminho do fio e o evento vai ser ou mudar ou adicionar, ou seja, o fio é criado pela primeira vez. E então, nós vamos fazer um loop aqui, apenas em caso nós estamos uploadando mais de um documento ao mesmo tempo. Então, o resto deste trabalho vai ser feito para cada fio que nós uploadamos ou editamos naquele segundo momento de publicação para os fios locais.

E então, a primeira coisa que nós queremos fazer é setar o metadata aqui. Então, o nosso fio id, que eu estou apenas usando o caminho, já que, ao contrário do Google Drive, nós não realmente temos um id verdadeiro. E então, para o tipo de fio e o título, apenas extraindo esse pouco de Javascript do nosso caminho.

E então, a primeira coisa que eu quero fazer antes de eu até inserir qualquer coisa na base de conhecimento, é que eu quero uma folha limpa. Se nós já adicionamos o fio para a base de conhecimento em algum ponto, eu quero eliminar tudo. E é isso que eu estou fazendo com esses dois nodos aqui.

Então, eu estou eliminando todos os antigos recordes de documentos da tabela document.pg, onde nós temos nossos embedimentos para RAG. E então, eu também estou eliminando para qualquer fio de mesa todas as linhas aqui na tabela document.rows. E a razão pela qual eu quero fazer isso é que eu quero absolutamente garantir que, quando eu atualizo um fio, a base de conhecimento somente contém a informação atualizada para aquele fio e não tem nada de uma versão antiga caindo por aí. Isso definitivamente faz o LLM alucinar.

Então, nós estamos tomando cuidado disso. E, a propósito, para todos os nodos de postgres na base de conhecimento do RAG, os credenciais vão ser os mesmos que você instalou quando você fez a instalação do database. E então, depois disso, agora nós podemos começar a inserir o nosso metadata document.

Então, essa é a nossa primeira inserção na base de conhecimento, onde nós estamos adicionando esse recorde, apenas começando com o ID, título e ato criado. E então, a esquema para as tabelas virá depois. E então, neste ponto, nós estamos apenas inserindo o título, como você pode ver bem ali.

E então, nós vamos para a inserção do recorde na base de conhecimento. E então, nós estamos trazendo esse recorde para a nossa instalação de N8n, para que nós possamos então extrair o texto dele. E é isso que nós estamos fazendo nesta parte mais básica do trabalho bem aqui.

Porque, baseado no recorde que nós estamos adicionando para a base de conhecimento, nós temos que extrair o conteúdo de formas diferentes. Você não pode ler um PDF do mesmo jeito que você lê um texto. Então, nós temos este estatamento de troca bem aqui, que, baseado no tipo do recorde, apenas olhando para a extensão do caminho do arquivo, se for um PDF, nós vamos para um caminho, se for um CSV, nós vamos para outro caminho, e então, se for um arquivo de texto, nós vamos para este caminho de baixo, como nós vemos na minha execução atual.

E este caminho de baixo é também o default. E então, se você tem algo como um arquivo de marcação, ele também pode ser tratado como apenas um arquivo de texto regular. Então, nós tratamos tipos de arquivos diferentes também.

E se você está curioso, se você for abrir e ir para um novo node aqui e clicar em extrair do arquivo, existem tipos de arquivos diferentes que são suportados do N8n. Então, se você quiser fazer algo como HTML ou JSON, você pode expandir este template para incluir esses tipos de arquivos também. Porque você tem todos esses nodes diferentes para lidar com esses tipos de arquivos diferentes.

E então, aqui neste execução, é só extrair o texto do documento. Então, nós pegamos o nosso arquivo, o dados binários, e nós recebemos o texto dele para realmente ter isso no N8n workflow para nodes seguintes. E então, nós pegamos esse texto e então, nós apenas colocamos ele em nossa base de conhecimento.

Nós usamos o PgVectorStore node para pegar todos os textos, dividi-los em pedaços e então colocá-los em nossa base de conhecimento no table Pg do documento, aqui. E então, para isso, nós temos que conectar um par de coisas. Nós temos o modelo de embedição de Olama, que eu estou apenas usando o NomecEmbedText.

Esse é o modelo que vem com o pacote local AI de default. E então, para o loader do documento, só usando o loader de dados de default, super simples. A única coisa que eu adicionei a isto, é que eu estou adicionando metadata para este recorde, incluindo o ID do documento e o título do documento.

E então, estes pedaços de informação estão disponíveis para nós no metadata JSONB. E isso é realmente importante, porque quando nós fazemos um lookup de RAG, quando este recorde é retornado, a metadata é incluída. O LLM pode, de fato, usar esta informação para localizar sua fonte.

Tipo, ele pode te dizer que eu peguei esta informação do documento de overview da empresa. Então, é por isso que nós nos importamos com metadata. E então, finalmente, para o text splitter aqui, para pegar nosso documento e transformá-lo em pedaços individuais, nós estamos apenas usando um basic, recomendado, recurso de personagens text splitter com um tamanho de 400 pedaços.

Eu peguei isto como recomendação no meu último vídeo, especificamente, para usar 400. Então, isso é tudo para um documento de texto básico. Então, para o nosso CSV e arquivos Excel, nós fazemos algo um pouco extra aqui, porque nós temos que inserir os pedaços para a tabela do documento também.

Então, primeiro, nós extraímos do Excel e CSV, assim como você faz com nossos documentos de texto e PDF. Mas nós fazemos duas coisas em paralelo. Primeiro, nós inserimos os pedaços de tabela.

Então, com um pouco de JavaScript fantástico aqui, eu pego toda a data para cada pedaço e eu insiro eles aqui no documento de pedaços. Então, nós temos este JSONB que tem toda a data de coluna para cada pedaço. E então, também, o que nós queremos fazer é adicioná-lo para a nossa base de conhecimento para regular RAG.

Porque, às vezes, um lookup RAG realmente funciona bem com pedaços, especialmente se você quiser olhar um pedaço específico. Então, mesmo que nós vamos primariamente estar usando os pedaços para escrever queries SQL para nossos dados tabulares, nós ainda queremos adicioná-lo para a base de conhecimento, assim como nossos outros fios mais baseados em texto. Então, nós agregamos todas as linhas juntos, então, nós a sumamos para que nós tenhamos a representação de texto da tabela inteira.

E então, assim como com o documento de texto, nós insirimos tudo para Postgres ou SoupBase com os pedaços e tudo assim. E então, a última coisa que nós fazemos é setar a esquema também. E então, isso está dentro da tabela de metade da documentação.

Nós criamos a esquema apenas extraindo todas as colunas desse arquivo e então, nós atualizamos esse recorde. Então, nós vamos de volta para o documento de metade. Lembre-se, mais cedo no workflow, nós já criamos esse recorde, só que não tinha a esquema ainda.

Então, agora, isso é onde nós adicionamos a esquema. nós dizemos para o feedback do cliente, aqui estão as colunas disponíveis, então, você sabe como escrever uma query SQL para escrever esse dado de linhas aqui. E é isso, esse é nosso pipeline RAG.

E então, agora, você literalmente pode ir e um monte de fios nesse arquivo que está sendo assistido, e você pode ir para a tabela de execução e ver ele carregar cada um desses fios individualmente para a base de conhecimento. E isso acontece muito, muito rápido. Mesmo com uma máquina não super poderosa, o Navic Embed Text Model de Malama é super rápido.

Esse trigger, ele vai instantaneamente trigger, ao contrário do Google Drive, onde você tem que pular a cada minuto. Então, em apenas alguns segundos, você terá um monte de fios já prontos na sua base de conhecimento, pronto para conversar com o agente e colocá-lo. Por último, vamos instalar o nosso agente AI.

Isso é tudo no amarelo e verde aqui. Então, primeiro de tudo, dois triggers para isso. Temos um chat trigger.

Isso é o que nos dá a capacidade de interagir com ele direto no N8nUI. E também temos um webhook, para que possamos usar o nosso agente como um ponto final do API. E então, eu tenho esses fios de edição aqui, apenas para standardizar a entrada que vem de ambos esses triggers.

E então, vamos para o nosso agente de ferramentas. E eu tenho um prompto de sistema muito básico aqui, apenas dizendo que é um assistente pessoal que tem essas diferentes ferramentas para um agente de ferramentas para olhar a base de conhecimento de diferentes maneiras, e eu lhe dou algumas instruções sobre quando usar os diferentes ferramentas também. E então, para a nossa memória de conversa aqui, apenas uma memória de conversa muito básica, as mesmas credenciales que você já instalou.

E então, ele automaticamente cria essa tabela de histórias de conversa N8n, assim como ele automaticamente cria a tabela de PDF dos documentos para nossas embedidas. E então, para o nosso modelo de conversa. Isso parece um pouco estranho aqui, tipo, por que há um ícone de OpenAI quando é tudo AI local? Não se preocupe, isso está usando o Ollama, é só dentro das minhas credenciales de OpenAI, eu coloquei a chave API para o que eu quero, é o Ollama, não importa.

E então, eu mudei a base URL para apontar para o meu conteineiro Ollama com o pacote local AI. E então, se você está usando Ollama Host na sua máquina, você só mudaria isso para localhost. Agora, a razão pela qual eu tenho que fazer isso, a razão por que eu não posso apenas usar o modelo de Ollama de conversa node, é um estranho glitch em N8n, onde se você usar esse cara, que eu gostaria que eu pudesse, em vez do OpenAI, se você usar esse, você vai ter um erro quando invoca o tool RAG, e eu já vi isso nos fóruns de comunidade, é algo como string esperado, mas objeto recebido, ou algum tipo de erro como esse, você vai ter isso quando você usar esse cara, então, não podemos usar isso por enquanto, e é um pouco desafortunado, eu vou expandir sobre isso mais depois, então, nós só temos que usar o OpenAI, e então, nós podemos mudar a base URL, então, ainda vamos poder carregar dinamicamente os modelos que temos puxados em Ollama, então, ainda funciona bem, e então, para nossos ferramentas aqui, primeiro, nós temos RAG, apenas com o nosso postgrasp pg vector, então, apenas uma descrição muito básica para o agente saber quando usar esse tool, você provavelmente poderia expandir esse prompt, apenas como você poderia com realmente qualquer um dos promptos neste template, é apenas para te fazer começar, mas sim, então nós temos nosso nome de mesa, documentos pg, limitando-se a quatro, e então nós estamos incluindo o metadata também, coisas como o título e a idade do arquivo que eu mostrei antes, para que o LLM possa localizar suas fontes quando ele usa esse tool RAG, e então, para as embedidinhas, apenas usando Ollama, apenas como nós vimos no pipeline RAG, o mesmo modelo exato, e então, credenciales para isso, de novo, você apenas referência o serviço Ollama se você está no paquete local AI, ou você muda isso para localhost se você está usando Ollama na sua própria máquina e não dentro de um conteineiro com o paquete local AI, e então, nós temos nossas últimas três ferramentas aqui, nós temos uma para listar nossos documentos, de novo, indo para a tabela de metadata dos documentos, e então, isso é apenas uma lista simples sobre essa tabela, não muito mais do que isso, e então, a ferramenta para obter os conteúdos de um arquivo específico também, então, nós temos esse tipo de query fantástica aqui, basicamente, só ir para a tabela PG dos documentos que a N8n cria para nós, tomando todos os pedaços para um único documento e combinando-os juntos para obter o texto completo para aquele documento, e então, nós temos um parâmetro aqui, então, o LLM decide o ID do arquivo, e então, tudo mais é predefinido no query SQL aqui, e então, é realmente um pouco diferente do último tool aqui, porque para o querying da tabela de dados, nós permitimos o LLM completamente gerar o query SQL, e isso é onde você pode obter um pouco mais de resultados inconsistentes, porque nós estamos requerindo para gerar uma lista inteira, e, às vezes, LLMs em particular, podem halucinar bad queries, mas, na descrição para o tool aqui, eu dou alguns exemplos, e eu encontrei que isso ajuda muito, então, a descrição do tool, isso é dado como parte do prompt do LLM, então, nós dizemos como usar esse tool, quando usar ele, e alguns exemplos de boas queries, tomando vantagem desta coluna de dados roda, então, basicamente, permitindo para entender como nós temos coisas estruturadas neste documento de tabelas rodas, então, pode escrever boas queries para referenciar tudo deste JSONB propriamente, e então, isso é tudo para nossos tools.

A última coisa que eu quero cobrar é a enorme dose de dados porque aqui está a coisa, as modelos de default, elas só tem uma lenta contexto de 2000 tokens, então, o que isso significa é que quando seus prompts ficam mais longos, quando você inclui mais informação de seus look-ups você começa a perder coisas, como o prompt e as instruções porque toda essa coisa no início do seu prompt é superado pelas mais usos de com agentes de A.I., então, olhando para o chat o lama modelo node, nós temos a opção aqui para mudar a lenta aqui na N8n, então, sim, aqui está a default, 2048, podemos mudar isso para 8000 ou o que for, mas, como vimos antes, nós não temos a luxúria de usar o lama chat modelo por causa desse glitch com RAG, então, nós temos que o A.I. apenas mudar a base URL, mas não tem uma opção aqui para mudar a lenta de contexto como nós temos no lama, máximo número de tokens, isso define o máximo para a saída, isso não muda a lenta de contexto, isso não funciona, eu realmente testei isso, então, a solução aqui, se você olhar os modelos que eu tenho para o lama, eu tenho os que eu instalado diretamente do

(Este arquivo tem mais de 30 minutos. Atualize para Ilimitado em TurboScribe.ai para transcrever arquivos de até 10 horas.)