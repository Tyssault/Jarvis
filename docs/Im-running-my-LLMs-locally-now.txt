I'm running my LLMs locally now!
 Transcrito por TurboScribe.ai. Atualize para Ilimitado para remover esta mensagem. 
Uma coisa que estou fazendo este ano, estou brincando ou experimentando, e também estou ativamente usando em várias aplicações, é trabalhar com modelos de AI abertos, dirigindo modelos de AI abertos localmente na minha máquina ou em alguns servidores que eu paguei. E aqui estou falando de modelos como os modelos Gemma 3 do Google, os modelos Lama do Meta, os modelos Quen e muitos outros modelos. Agora, caso você não saiba, modelos de linguagem aberta são essencialmente os mesmos como os proprietários, os modelos GPT de OpenAI, por exemplo, ou os modelos Gemma 3 do Google.

A diferença importante é que com esses modelos abertos, você tem acesso aos seus preços, aos parâmetros, ou seja, à saída do processo de treinamento no final, e você pode, portanto, os gerar em seu próprio hardware, no seu computador, no seu computador de tablet, ou em um servidor que você pagou, por exemplo. Agora, é só possível, porque a maioria dos modelos abertos são pequenos do que os grandes proprietários. Mas não todos eles, por exemplo, DeepSeek R1 é aquele um modelo aberto que abriu toda a indústria mais cedo este ano, e é um grande modelo com mais de 600 bilhões de parâmetros, então não todos eles são pequenos, mas muitos deles estão no lado menor, e eles precisam ser, porque senão você não seria capaz de os gerar no seu computador, no seu sistema.

Agora, você, claro, pode se perguntar o quão bem um modelo tão pequeno pode funcionar, já que é bem menor do que os modelos gigantescos. Não sabemos os parâmetros de todos os modelos proprietários, mas podemos assumir que eles são bem grandes também, e a resposta surpreendente pode ser que, de fato, esses modelos abertos pequenos funcionam bastante bem. Você pode dar uma olhada em benchmarking, claro, você deve tomar isso com um pouco de sal, porque você pode optimizar modelos para benchmarking, e, claro, os modelos proprietários só publicam os números de benchmarking que eles gostam, mas você também pode dar uma olhada em coisas como o leaderboard da Arena LM, onde pessoas como você e eu basicamente votam modelos para cima ou para baixo, e aqui você veria, por exemplo, que o Gemma Model, o Gemma Model de 27 bilhões de parâmetros, funciona bastante bem, por exemplo.

Ele está no topo, apesar de ser muito, muito pequeno do que os grandes modelos proprietários. Agora, claro, ele não é o número um, então, claro, dependendo do uso que você tenha, usar um modelo tão aberto não pode ser uma alternativa viável, mas o meu argumento, e o que eu vi nos últimos meses, é que, para muitos casos de uso, eles são absolutamente perfeitos, e eles trazem alguns grandes vantagens para a mesa. Então, por exemplo, se estamos falando de coisas como análise de dados, extração de informação de documentos ou geração de conteúdo, especialmente quando combinado com promulgação de ViewShot, esses modelos são incríveis, e o grande vantagem não é apenas que você não tem que pagar por eles se você deixar seu hardware de lado, mas que você tem 100% de privacidade.

E, claro, você pode não se importar com privacidade. Você pode estar bem com enviar seus promotores para OpenAI ou qualquer outro providor, e com eles usando esses promotores e as saídas para treinamento, e eu não me preocupo com isso também, para ser honesto, mas eu não estou superinteressado em enviá-los todos meus documentos de confiança, para ser honesto. E não é apenas o treinamento.

Talvez eles me digam que eles não treinarão baseado em meus dados, mas eu realmente não quero compartilhar certos documentos com eles, de forma alguma. Eu não quero ter meus documentos privados ou imagens, necessariamente, em seus servidores. E é aí que esses modelos abertos realmente se usam, porque se eu gerar um modelo aberto na minha máquina local, e fazer isso é supersimples, eu vou voltar a isso, se eu gerar isso na minha máquina local, então, é claro, meus dados nunca saem da minha máquina, nunca chegam ao criador desse modelo, nunca são usados para treinamento ou expostos em lugar nenhum.

Além disso, é claro, esse modelo também funciona se eu não tenho conexão com a internet, em um avião, por exemplo, e ser capaz de perguntar perguntas sobre código lá é melhor do que não ser capaz de perguntar em geral. E eu tenho zero latência. E se os servidores do OpenAI estão desligados de novo, bem, então eu também não tenho que me preocupar com isso, porque o meu modelo na minha máquina ainda está funcionando.

Além disso, além disso, você também não tem nenhum login de vendedor. Se o OpenAI desenrola uma nova versão, e eu estou apenas usando eles como um exemplo, é o mesmo para o Google e assim por diante, se eles desenrolam uma nova versão de algum modelo e de repente aquele modelo funciona pior do que antes, ou se eles limitam o meu valor ou qualquer coisa mais, não há nada que eu possa fazer sobre isso, certo? Eu posso cancelar minha inscrição e ir para um outro providor, mas é sobre isso. Com o meu modelo que está funcionando localmente, isso não é algo que eu tenho que me preocupar com, porque enquanto eu não o replaço, ele não vai mudar.

É o mesmo modelo de amanhã como era ontem. Ele não vai ser atualizado magicamente ou algo assim. Então isso é tudo incrível, e é por isso que eu estou realmente usando esses modelos abertos para muitos, muitos casos de uso.

Agora, usá-los é bem simples, graças a Deus, porque há ferramentas como Olama e LM Studio, que são extremamente user-friendly, especialmente LM Studio, porque é realmente apenas uma aplicação que você instala, está disponível para Windows, MacOS, Linux. Isso te dá uma boa interface de uso gráfico e você pode descarregar e gerenciar seus modelos direto do interior daquela ferramenta. Você pode conversar com esses modelos, atingir fios.

Se você tem um modelo que o apoia, você também pode atingir imagens. Por exemplo, os três modelos de Gemma que são oferecidos pela Google, e, a propósito, esses modelos de Gemma não devem ser confusos com os modelos de Gemini. Os modelos de Gemma são gratuitos, os modelos de Gemini não são.

Então, essa é uma diferença importante. E, portanto, essa é uma maneira incrível de usar esses modelos, melhor do que tudo. LM Studio e Olama também te permitem usar esses modelos programatizadamente, porque ambos expõem um API que, então, está funcionando localmente no seu sistema.

Ou, se você estivesse a funcionar Olama em um servidor rentado, você também poderia expor os portos relevantes ou oferecer pedidos para o API que está funcionando no servidor, para que você pudesse se comunicar com ele pela sua máquina local, ou seja. Não importa como você a instala, você pode se comunicar com esses APIs providos pelo LM Studio ou Olama com seus modelos de AI localmente dentro do seu código. E isso é algo que eu estou usando muito para construir todos os tipos de automação que funcionam no meu sistema e que usam esses modelos localmente funcionando.

E, como você pode ouvir, eu estou muito animado com isso. E é por isso que eu criei um curso completo sobre isso. Você encontrará um link com um bom desconto abaixo deste vídeo.

E, neste curso, eu compartilho todo o conhecimento e experiência que eu coloquei relacionados a esses modelos abertos nos últimos meses. Eu mostro em detalhe como instalar esses ferramentas, como configurá-los, como instalar modelos, como modificar modelos, como ajustar configurações de modelos, como construir modelos customizados baseados em modelos baseados. Pulemos em exemplos de uso, exemplos de código, como usar esses modelos programaticamente e muito, muito mais.

Também exploramos quantização, que é uma técnica que garante que você possa gerar modelos ainda mais sizáveis em endereços baixos ou em hardware não-top-notch. Então, isso é superimportante. E, sim, portanto, se você também quiser alcançar todos esses benefícios e você não quer mergulhar em todos os docs sozinho e descobrir tudo sozinho, aproveite o meu conhecimento.

Eu compartilhei aqui. Você encontra um link com um preço muito acessível para esse curso abaixo deste vídeo. Mas, não importa se você tira esse curso ou não, eu posso realmente recomendar brincar com esses modelos abertos e ver se você também tem alguns casos de uso que você pode aproveitar deles.

Agora, você não é provável de cancelar sua inscrição no ChatGPT ou no Google Gemini por causa desses modelos. Eu também não fiz. Eu os usei em conjunção.

Mas, como eu expliquei, para certos casos de uso, para certas automações, essas são alternativas incríveis e mais baratas, já que eu não tenho que pagar para a usagem do API OpenAI, por exemplo, ao contrário de apenas usar o ChatGPT ou os modelos do OpenAI GPT. Então, dê uma pensada. E, se você quiser, dê uma pensada ao curso.

Isso realmente vai te ajudar a começar a usar esses modelos abertos localmente.
 Transcrito por TurboScribe.ai. Atualize para Ilimitado para remover esta mensagem.
