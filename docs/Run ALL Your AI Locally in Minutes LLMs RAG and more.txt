Run ALL Your AI Locally in Minutes (LLMs, RAG, and more)
 Transcrito por TurboScribe.ai. Atualize para Ilimitado para remover esta mensagem. 
Você já desejou um único paquete que você pode facilmente instalar que tem tudo o que você precisa para local.ai? Bem, eu tenho boas notícias para você hoje porque eu tenho exatamente o que você está procurando. Na verdade, eu nunca estive tão animado para fazer um vídeo sobre algo antes. Hoje, eu vou te mostrar um incrível paquete para local.ai desenvolvido pelo time da N8n.

E essa coisa tem tudo. Ela tem Ollama para os LLMs, Quadrant para o database de vetores, Postgres para o database de SQL e N8n para unir tudo junto com automação de trabalho. Essa coisa é absolutamente incrível e eu vou te mostrar como instalar em apenas minutos.

Depois, eu vou até mostrar como expandi-la para fazê-la melhor e usá-la para criar um agente de A.I. completo em N8n. Então, fique atento porque eu tenho um monte de valor para você hoje. Runir sua própria infraestrutura de A.I. é o caminho para o futuro, especialmente por causa de como está se tornando acessível e porque modelos de abertura de fonte, como LLama, estão chegando ao ponto em que eles são tão poderosos que eles são capazes de competir com modelos de abertura de fonte, como GPT e Clon.

Então, agora é a hora de pular nisso e o que eu estou a mostrar para você é um ótimo começo para isso. E no final deste vídeo, eu vou até falar sobre como eu vou expandir este paquete no futuro próximo apenas para você, para fazê-lo ainda melhor. Tudo bem, então aqui estamos no repositório GitHub para o kit de iniciador de A.I. self-hostado pela N8n.

Agora, este repo é realmente, realmente básico e eu adoro. São basicamente apenas dois arquivos que temos que se importar aqui. Temos o arquivo de variáveis ambientais, onde vamos setar credenciais para coisas como Postgres, e depois temos o docker-compose, este arquivo YAML aqui, onde basicamente vamos trazer tudo junto, como Postgres, Quadrant e LLama, para ter um único paquete para nossa A.I. local.

Agora, a primeira coisa que eu quero mencionar aqui é que este readme tem instruções para como instalar tudo sozinho, mas honestamente, é bastante faltante e há um par de buracos que eu quero encher aqui com maneiras para estendê-lo, para realmente fazê-lo o que eu acho que você precisa. Então, eu vou passar por isso em um pouco e realmente vamos ter isso instalado no nosso computador. Agora, há um par de dependências antes de você começar.

Basicamente, você só precisa de Git e docker. Então, eu recomendo instalar GitHub Desktop e depois docker-desktop também, porque isso também tem docker-compose com ele, o que é o que precisamos trazer tudo junto para um paquete. Então, com isso, podemos ir em frente e começar a descarregar isso no nosso computador.

Então, a primeira coisa que você quer fazer para descarregar este código é copiar o comando git-clone aqui, com a URL do repositório. Você vai para um terminal, então, e depois paste este comando. Para mim, eu já clonei isso, é por isso que eu recebo esta mensagem de erro, mas você vai ter este código descarregado no seu computador e depois você pode mudar seu diretório para este novo repositório que você puxou.

Então, com isso, nós podemos agora ir e editar os fios em qualquer editor da nossa escolha. Eu gosto de usar código VS, então, se você tem código VS também, você pode apenas escrever em code. E isso vai puxar tudo em Visual Studio Code.

Agora, as instruções oficiais no ReadMe que nós acabamos de ver, vão te dizer, neste ponto, para executar tudo com o comando docker compose. Agora, isso não é, na verdade, o próximo passo certo. Eu não sei muito bem por que eles dizem isso, porque nós temos que, na verdade, ir e editar algumas coisas no código para fazê-lo customizado para nós.

E isso começa com o .env fio. Então, você vai querer ir para o seu .env fio. Eu apenas fiz um .env.example fio neste caso, porque eu já tenho o meu credencial instalado.

Então, você vai para o seu .env e depois instala o seu nome e passado do Postgres, o nome do database, e também um par de segredos N8n. Estes podem ser o que você quiser, apenas faça com que eles sejam muito, muito seguros e, basicamente, apenas um fio alfanumérico. E então, com isso, nós podemos ir para o nosso fio docker compose.

E aqui é onde eu quero fazer um par de extensões para realmente encher o espaço, então, um par de coisas que estávamos perdendo no fio original docker compose. Primeiramente, por algum motivo, o conteineiro do Postgres não tem o portão exposto por default. Então, você não pode realmente ir e usar o Postgres como seu database em um fio N8n.

Eu acho que o N8n usa o Postgres internamente, o que é por isso que está instalado assim inicialmente, mas nós queremos realmente poder usar o Postgres para a nossa memória de conversa, para nossos agentes, então, eu vou te mostrar como fazer isso. Basicamente, tudo o que você tem que fazer é ir para o serviço Postgres aqui e então apenas adicionar estas duas linhas de código aqui. Portões e então apenas um item onde nós temos 5, 4, 3, 2 mapado para o portão 5, 4, 3, 2 dentro do conteineiro.

Dessa forma, nós podemos ir ao portão local 5, 4, 3, 2 e acessar o Postgres. Então, isso é super, super importante, senão, nós não vamos realmente poder acessá-lo dentro do N8n Workflow e nós vamos fazer isso mais tarde quando nós construirmos o agente AI RAG. Agora, a outra coisa que nós queremos fazer é que nós queremos usar o Ollama para as nossas embedidas para o nosso database de vetores também.

Agora, a comandante base, quando nós inicializamos o Ollama, é apenas esta parte aqui. Então, nós dormimos por 3 segundos e então nós pulamos o Llama 3.1 com o Ollama. Então, é por isso que nós temos o Llama 3.1 disponível para nós, por defaulto.

Mas o que eu adicionei aqui é outra linha para pular um dos modelos de embedida do Ollama. E nós precisamos disso se nós queremos ser capazes de usar o Ollama para o nosso RAG. Então, eu adicionei essa linha também.

Isso é muito, muito importante. Então, isso é literalmente tudo que você tem que mudar no código para fazer isso funcionar. E eu até vou ter um link na descrição deste vídeo para a minha versão disso.

Você pode pular isso diretamente se quiser para ter todas as customizações que nós acabamos de fazer aqui. E com isso, nós podemos começar com o Docker Compose. Então, as instalações, as instalações e o README são bem úteis aqui porque há um comando um pouco diferente do Docker Compose que você quer usar baseado na sua arquitetura.

Então, se você tem um GPU NVIDIA, você pode seguir essas instalações que são um pouco mais complicadas, mas se você quiser, você pode. E então, você pode usar um GPU NVIDIA Profile. E então, se você é um usador Mac, você segue este comando aqui.

E então, para todos os outros, como o que eu vou usar neste caso, mesmo que eu tenha um GPU NVIDIA, eu só vou mantê-lo simples com docker-compose-profile-cpu-up. E então, nós vamos copiar este comando, ir para o nosso terminal aqui e pastá-lo. E no meu caso, eu já criei todos estes contêineres.

Então, vai funcionar muito, muito rápido para mim. Mas no seu caso, você vai ter que pular cada imagem para o Llama, Postgres, N8n e Quadrant. E então, comece-os todos e vai levar um pouco porque eu também tenho que fazer coisas como pular o Llama 3.1 para o contêiner do Llama.

E então, no meu caso, vai passar bem rápido porque já fiz bastante disso. Eu fiz isso de propósito, então pode ser um passeio mais rápido para você aqui. Mas você pode ver todos os diferentes contêineres, os diferentes cores aqui, que estão funcionando tudo para me setar para cada um dos diferentes serviços.

E então, aqui, por exemplo, pulou o Llama 3.1 e aqui pulou o modelo de embedição que eu escolhi do Llama também. E então, neste ponto, está basicamente feito. Então, vou pausar aqui e volto quando tudo estiver pronto.

Tudo bem, então tudo está pronto e agora eu vou levar você em um docker para que possamos ver tudo isso funcionando ao vivo. Então, você vai querer abrir seu desktop docker e então você vai ver um recorde aqui para o self-hosted AI Starter Kit. Você pode clicar neste botão na esquerda para expandi-lo e então podemos ver cada contêiner que está atualmente funcionando ou foi para o set-up.

Então, vão ser quatro contêineres, cada um funcionando para um dos nossos diferentes serviços de AI locais. E podemos, na verdade, clicar em cada um deles, o que é super legal porque podemos ver a saída de cada contêiner e até ir para o tab Exec para gerar comandos Linux dentro de cada um destes contêineres. Então, você pode fazer coisas em tempo real também sem ter que reiniciar o contêiner.

Então, você pode ir para o contêiner Postgres e gerar comandos para queryar suas tabelas e tal. Você pode ir para... Na verdade, eu vou mostrar isso muito rapidamente. Você pode ir para o contêiner Ollama e você pode pular em tempo real.

Tipo, se eu quiser ir para Exec aqui, eu posso fazer Ollama, pular, Ollama 3.1, se eu posso espelhar bem, 70B. Então, eu posso pular modelos em tempo real e ter eles atualizados e disponíveis para mim no N8n sem ter que realmente reiniciar qualquer coisa, o que é super, super legal. Tudo bem.

Então, agora é a parte realmente divertida, porque nós podemos usar toda a infraestrutura local que nós espalhamos agora para criar um agente RAG-AI dentro do N8n. Então, para acessar o seu novo N8n você pode ir para localhost port 5678. E a forma que você sabe que esta é a URL é através dos logs do Docker para o seu contêiner N8n ou no Readme que nós passamos que estava no repositório GitHub que nós clonamos.

E com isso, nós podemos descer nesse funcionamento que eu criei para usar Postgres para a memória de chat, Quadrant para RAG e Ollama para o LLM e o modelo de embedimento. Então, este é um agente RAG-AI completo que eu já criei. Eu não quero construir de novo, só porque eu quero que isso seja um passeio mais rápido e suave para você.

Mas eu ainda vou passo a passo por tudo o que eu coloquei aqui para que você possa entender por si mesmo e também apenas roubar isso de mim, porque eu vou ter isso no link de descrição também, para que você possa pular esse funcionamento e levá-lo para sua própria instância N8n. E com isso, nós podemos ir em frente e começar. Então, há duas partes para esse funcionamento.

Primeiramente, nós temos o agente em si com a interação de chat aqui. Então, este widget de chat é como nós podemos interagir com nosso agente. E então, nós também temos o funcionamento que vai trazer fios do Google Drive para nossa base de conhecimento com Quadrant.

Então, eu vou mostrar o agente primeiro e então eu vou pular muito rapidamente para como eu tenho esse setup de pipeline para pular fios de um fio do Google Drive para minha base de conhecimento. Então, nós temos o trigger que eu acabei de mencionar lá, onde nós temos nosso input de chat e isso é fedido diretamente para esse agente AI, onde nós conectamos todas as coisas locais diferentes. E então, primeiro de tudo, nós temos nosso modelo de chat Ollama.

E então, eu estou referenciando llama 3.1 colon latest, que é o modelo de 8 bilhões de parâmetros. Mas se você quiser fazer um pulo de Ollama dentro do conteineiro, como eu mostrei como fazer, você pode usar literalmente qualquer Ollama LLM aqui. É tão, tão simples para setar.

E então, para as credências aqui, é muito fácil, você só tem que colocar essa url de base aqui. É tão importante que para a url, você usa HTTP e em vez de localhost, você referencia host.docker.internal, senão não vai funcionar. E então, o porto para Ollama é, se você não muda, 1143.4 e você pode pegar esse porto tanto no arquivo docker compose ou nos logs para o conteineiro Ollama.

Você verá isso em muitos lugares. E então, com isso, nós temos o nosso LLM preparado para esse agente e então, para a memória, é claro que nós vamos usar Postgres. Então, eu vou clicar nisso e nós vamos ter qualquer tipo de nome de mesa que você tiver aqui e nós vamos criar isso automaticamente no seu database Postgres e ele vai receber o ID de sessão do node anterior e então, para as credências aqui, isso vai ser baseado no que você setou no seu .env fio.

Então, nós temos o nosso host, que é host.docker.internal de novo, igual ao Ollama e então o nome do database, o usuário e o password, todos três desses, você definiu no seu .env fio que você passou antes e o porto para Postgres é 5432. E então, com isso, nós temos a nossa memória de conversa local preparada. É tão simples.

E então, nós podemos passar para a última parte desse agente, que é a ferramenta para RAG. Então, nós temos a ferramenta VectorStore que nós conectamos ao nosso agente e então, nós conectamos o nosso QuadrantVectorStore para isso. E então, nós vamos retirar qualquer documento baseado na query que vem para o nosso agente e então, para os credenciais para o Quadrant, nós só temos uma chave API, que este foi filtrado para mim, por default, então, espero que seja para você também.

Eu acho que é apenas o password para a instância N8n e então, para a URL Quadrant, isso deve parecer muito, muito familiar. http host.docker.internal e então, o porto para Quadrant é 6333. De novo, você pode pegar isso do arquivo do Docker Compose porque nós temos que exposar aquele porto, fazer-o disponível ou você pode pegá-lo dos logs Quadrant também.

E então, outra coisa que eu quero mostrar que é muito, muito legal com a hostagem Quadrant localmente é que se você for para localhost port 6333, como eu tenho aqui, você pode ver no topo esquerdo, slash dashboard, isso vai levá-lo para o seu próprio Quadrant Dashboard onde você pode ver todas as suas coleções, sua base de conhecimento, basicamente, e você pode ver todos os diferentes vetores que você tem lá, você pode clicar em visualizar e eu posso ir e ver todos os meus diferentes vetores, esse é um documento que eu já tenho inserido enquanto eu estava testando coisas, então você pode ver todas as metadatas, os conteúdos de cada pedaço, isso é tão, tão legal, então vamos voltar a isso em um pouco aqui, mas apenas saiba que você tem tanta visibilidade para sua própria instância Quadrant e você pode até ir e, tipo, fazer suas próprias query para pegar coleções ou deletar vetores ou fazer uma pesquisa, é simplesmente muito legal. Então, sim, hostar Quadrant é uma coisa linda. E então, com isso, nós temos o nosso Quadrant Vector Store e então nós estamos usando Olama para embedimentos usando aquele modelo de embedimento que eu pulei, que eu adicionei para o arquivo do Docker Compose e então nós vamos usar Olama 3.1 de novo para parcerizar as respostas que nós recebemos da RAG quando fazemos nossos lookups.

Então isso é tudo para o nosso agente e então vamos testar isso em um pouco, mas primeiro eu quero realmente mostrar o funcionamento para ingerir arquivos para a nossa base de conhecimento e então a forma que isso funciona é que nós temos dois triggers aqui, basicamente, quando um arquivo é criado em um folder específico no Google Drive ou se um arquivo está atualizado nesse mesmo folder, nós queremos gerar esse pipeline para descarregar o arquivo e colocá-lo no nosso Quadrant Vector Database funcionando localmente e então esse folder que eu tenho aqui é esse folder de notas de encontro no meu Google Drive e especificamente o documento que eu vou usar para propósitos de teste aqui são essas notas de encontro falsas que eu fiz, eu apenas gerentei algo realmente, realmente estúpido aqui sobre uma empresa que está vendendo pets robóticos e startup de AI então nós vamos usar esse documento para nosso array eu não vou fazer um monte de documentos diferentes porque eu quero manter isso realmente simples agora, mas você pode definitivamente fazer isso no Quadrant Vector Database pode lidar com isso, mas por enquanto eu estou apenas usando esse único documento e então eu vou passar passo a passo o que esse fluxo realmente parece para ingerir isso para o Vector Database e então, primeiro de tudo, eu vou pegar um evento de teste, que vai ser a criação desse folder de notas de encontro que eu apenas mostrei e então nós vamos gerar isso nesse nodo aqui, que vai extrapolar algumas partes importantes de informação, incluindo a idade do arquivo e a idade do folder e então, uma vez que temos isso, eu vou ir para esse próximo passo bem aqui e esse é um passo muito, muito importante ok, deixe-me parar aqui por um segundo há um monte de tutoriais de RAG com N8n no YouTube que percam isso quando você tem esse passo no final aqui, eu vou passar para o final bem rápido seja se isso é Super Base, Quadrant, Pinecone, não importa quando você tem esse inserador ele não é um Upsert ele é apenas um inserador e então o que isso significa é que se você reinsertar um documento você vai ter vectores duplicados para aquele documento então se eu atualizar um documento no Google Drive e ele reinserta os vectores para o meu Quadrant Vector Database eu vou ter os velhos vectores para a primeira vez que ingeri o meu documento e os novos vectores para quando eu atualizei o arquivo ele não se separa dos velhos arquivos ou atualiza os vectores em lugar isso é tão importante para se lembrar e então eu estou dando um monte de valor para você aqui incluindo esse node e é na verdade código custom porque não há um jeito de fazer sem código em N8n mas é tudo bom porque você pode apenas copiar isso de mim eu vou ter um link para esse trabalho na descrição como eu disse então você pode apenas descarregar isso e trazê-lo para o seu próprio N8n pegar meu código aqui que basicamente apenas usa LangChain para se conectar para o meu Quadrant Vector Store pegar todos os vectores ids onde o metadata file id é igual para a id do arquivo que estou atualmente ingerindo e então ele apenas descarrega esses vectores então basicamente nós descarregamos tudo que está atualmente no database de vectores para esse arquivo para que nós possamos reinsertá-lo e fazer com que nós tenhamos zero duplicações isso é tão importante porque você não quer versões diferentes do seu arquivo existindo ao mesmo tempo em sua base de conhecimento isso vai confundir o caralho do seu LLM então isso é um muito importante passo e então eu vou esse número foi nove quando eu primeiro mostrei esse dashboard quadrante e agora é zero mas vai voltar para nove quando eu terminar esse trabalho então próximo nós vamos descarregar esse arquivo Google Drive legal e simples então nós vamos extrair o texto dele então isso não importa se é um PDF um CSV um Google Doc ele vai pegar o arquivo e pegar o texto e então nós vamos inserir ele em nosso quadrante vector vendedor e então agora eu vou executar o passo de teste aqui e nós vamos voltar para a UI após ter terminado fazendo essas inserções você pode ver aqui nove itens porque ele truncou meu documento então nós vamos voltar aqui e eu vou refrescar agora é zero eu vou refrescar e aí nós vamos boom nós estamos de volta para nove truncadas e a razão há tantas truncadas para um pequeno documento é porque se você ir para meu tamanho de truncada aqui em meu recurso personagem text splitter eu tenho um tamanho de truncada de 100 então a cada vez que eu coloquei um documento vai ser dividido em 100 truncadas de personagem então eu quero mantê-lo pequeno apenas porque eu estou dirigindo lama 3.1 localmente eu não tenho o computador mais poderoso então eu quero meus prompts para ser pequeno então eu estou mantendo meu contexto menor por ter realmente rapidamente aqui é o meu documento carregador de dados e então ou meu carregador de dados de default eu estou adicionando dois pedaços de metade aqui o id e o folha id o mais importante aqui é o id porque isso é como eu sei que um vector está ligado a um documento então eu posso usar isso nesse outro passo aqui para deletar o velho documento vectores antes de inserir o novo então é assim que eu faço essa conexão ali então isso é tipo a parte mais em detalhe desse passeio é como isso tudo funciona e ter esse código customizado aqui mas apenas saiba que isso é tão importante então apenas tira isso de mim eu espero que isso faça sentido para um extento eu passei um monte de tempo fazendo isso funcionar para você então sim com isso isso é tudo nós temos nosso agente completamente instalado tudo ingerido em nós temos o documento atualmente na base de conhecimento porque eu passei passo a passo e então agora nós podemos ir em frente e testar essa coisa então eu vou ir para o chat widget aqui na verdade eu vou salvá-lo primeiro e então ir para o chat widget e então eu vou perguntar uma pergunta que ele só pode responder se ele realmente tem o documento na base de conhecimento e pode retirá-lo então eu direi o que é a campanha de ad focada em e porque isso é lama 3.1 correndo localmente vai realmente demorar um pouco para receber uma resposta porque eu não tenho o computador mais tudo bem então nós recebemos uma resposta de lama 3.1 e isso está parecendo bem bom é um pouco esquisito no começo da resposta aqui mas isso é apenas a saída rá sem nenhuma instrução de mim para o modelo sobre como formar uma resposta então você pode muito, muito facilmente fixar isso apenas adicionando para o prompto de para o LLM e dizendo como responder com a informação que é dado da RAG Google Drive então isso está funcionando absolutamente linda agora eu provavelmente quero fazer muito mais testes com esse conjunto inteiro mas só para manter as coisas simples agora vou deixar isso como um exemplo mas sim, eu encorajo você a apenas levar isso para frente continuar trabalhando nesse agente e sim, isso é completamente local é apenas uma coisa linda então eu espero que esse conjunto local de AI seja tão legal para você como é para mim porque eu estou tendo um abraço com isso e eu continuarei enquanto eu continuo expandindo nele então apenas como eu prometi no começo do vídeo eu quero falar um pouco sobre como estou planejando expandir isso no futuro para fazê-lo ainda melhor porque aqui está a coisa esse conjunto inteiro que eu mostrei aqui é um ponto de início realmente bom mas há algumas coisas que eu quero adicionar a ele também para fazê-lo ainda mais robusto coisas como Red é para cachar ou uma super base própria em vez do Vanilla Postgres porque então eu posso fazer isso eu incluir coisas como a fronte também ou talvez fazer as melhores práticas para Reds e LLMs ou qualquer e trabalho para isso para fazer isso mais como um template também para realmente fazer isso realmente realmente fácil para começar com local AI então eu espero que você esteja animado sobre isso se você está ou se você encontrou este vídeo apenas ajudoso em geral para você se equiparar com sua local AI tec stack
 Transcrito por TurboScribe.ai. Atualize para Ilimitado para remover esta mensagem.
